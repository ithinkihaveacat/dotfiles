#!/usr/bin/env bash

set -euo pipefail

# Bash 4.x version guard
if ((BASH_VERSINFO[0] < 4)); then
  echo "$(basename "$0"): requires bash 4.0 or higher (found ${BASH_VERSION})" >&2
  exit 1
fi

usage() {
  cat <<EOF
Usage: $(basename "$0") [TOPIC_FOCUS] < INPUT_FILE

Identifies "Validated Unknowns": Information in the input text that is strictly
novel and provably unknown to the target model (Gemini 2.5 Flash).

Arguments:
  TOPIC_FOCUS  Optional. A specific area to focus the questions on.

Input:
  stdin        The reference material (text) to analyze.

Options:
  -h, --help   Display this help message and exit

Environment:
  GEMINI_API_KEY  Required. Your Gemini API key.

Examples:
  cat documentation.md | $(basename "$0") "Security"
EOF
  exit 0
}

if [[ "${1:-}" == "-h" || "${1:-}" == "--help" ]]; then
  usage
fi

require() { hash "$@" || exit 127; }

require curl
require jq

USER_PROMPT="${1:-"Generate challenging questions based on novel information."}"
TARGET_MODEL="gemini-2.5-flash"
EVALUATOR_MODEL="gemini-3-pro-preview"

# Read stdin
if [ -t 0 ]; then
  echo "$(basename "$0"): Error: No input provided on stdin." >&2
  usage
fi
INPUT_DATA=$(cat)

if [[ -z "${GEMINI_API_KEY:-}" ]]; then
  echo "$(basename "$0"): GEMINI_API_KEY environment variable not set" >&2
  exit 1
fi

# -----------------------------------------------------------------------------
# STEP 1: Generate Questions (JSON) using Evaluator Model
# -----------------------------------------------------------------------------
echo "Thinking... (Generating candidate questions)" >&2

GEN_SYSTEM_INSTRUCTION=$(cat <<'EOF'
Analyze the provided reference material and generate exactly 7 questions that verify whether the text contains genuinely novel, unexpected, or counter-intuitive information.

### Core Task
Generate questions that satisfy the "Gotcha" condition:
**An expert in the general field relying on standard industry knowledge would likely answer incorrectly, but the correct answer is explicitly contained in the provided text.**

### CRITICAL: Context & Fairness
*   **Self-Contained Questions:** The questions will be asked to a model *without* the provided text. You MUST include all necessary context in the question itself.
*   **Specify Versions/Platforms:** If the information is specific to "Wear OS 5" or "Jetpack Tiles 1.4", you **must** explicitly state this in the question.
    *   *Bad:* "What is the limit for animated elements?" (Which system? Which version?)
    *   *Good:* "In Wear OS Tiles API v1.4, what is the hard limit on simultaneously animated elements?"

### Constraints & Mandates
1.  **Source Material Restrictions:**
    *   **Ignore Code Snippets:** Do not formulate questions based on code blocks, variable names, or syntax.
    *   **Ignore Illustrative Examples:** Do not use "For example..." scenarios or sample values.
    *   **Focus on Normative Text:** Derive questions only from the main narrative defining core rules, architecture, and behaviors.

2.  **Question Quality:**
    *   **Concept over Trivia:** Ask about architectural patterns, data flow, logical constraints, or counter-intuitive behaviors.
    *   **The "Gotcha" Factor:** The question should be difficult because it contradicts standard intuition.
    *   **Prioritization:** Select the 7 questions MOST likely to result in an incorrect response from an expert.

If the text contains only general knowledge, return an empty list.
EOF
)

# JSON Schema for Step 1
GEN_SCHEMA='{
  "type": "array",
  "items": {
    "type": "object",
    "properties": {
      "question": { "type": "string" },
      "answer": { "type": "string", "description": "The correct answer based strictly on the text." },
      "rationale": { "type": "string", "description": "Why this is counter-intuitive." }
    },
    "required": ["question", "answer", "rationale"]
  }
}'

GEN_REQUEST_BODY=$(jq -n \
  --arg system "$GEN_SYSTEM_INSTRUCTION" \
  --arg input "$INPUT_DATA" \
  --arg prompt "$USER_PROMPT" \
  --argjson schema "$GEN_SCHEMA" \
  '{
    system_instruction: { parts: [{ text: $system }] },
    contents: [{
      parts: [
        { text: ("Reference Material:\n\n" + $input) },
        { text: ("\n\nTask:\n" + $prompt) }
      ]
    }],
    generationConfig: {
      responseMimeType: "application/json",
      responseJsonSchema: $schema,
      temperature: 0.5
    }
  }')

GEN_RESPONSE=$(echo "$GEN_REQUEST_BODY" | curl -s -X POST \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -d @- \
  "https://generativelanguage.googleapis.com/v1beta/models/${EVALUATOR_MODEL}:generateContent")

# Extract JSON array of questions
QUESTIONS_JSON=$(echo "$GEN_RESPONSE" | jq -r '.candidates[0].content.parts[0].text // empty')

if [[ -z "$QUESTIONS_JSON" ]]; then
  echo "$(basename "$0"): Failed to generate questions." >&2
  echo "Response: $GEN_RESPONSE" >&2
  exit 1
fi

# -----------------------------------------------------------------------------
# STEP 2 & 3: Loop, Test, and Validate
# -----------------------------------------------------------------------------

TMP_DIR=$(mktemp -d)
trap 'rm -rf -- "$TMP_DIR"' EXIT
VALIDATED_FILE="$TMP_DIR/validated.md"
REJECTED_FILE="$TMP_DIR/rejected.md"
touch "$VALIDATED_FILE" "$REJECTED_FILE"

mapfile -t QUESTION_ITEMS < <(echo "$QUESTIONS_JSON" | jq -c '.[]')

for ITEM_JSON in "${QUESTION_ITEMS[@]}"; do
  QUESTION=$(jq -r '.question' <<< "$ITEM_JSON")
  ANSWER=$(jq -r '.answer' <<< "$ITEM_JSON")
  RATIONALE=$(jq -r '.rationale' <<< "$ITEM_JSON")

  echo "Validating: $QUESTION" >&2

  # --- Test: Ask Target Model (No Context) ---
  TEST_BODY=$(jq -n \
    --arg q "$QUESTION" \
    '{
      contents: [{ parts: [{ text: $q }] }]
    }')
  
  TEST_RESPONSE=$(echo "$TEST_BODY" | curl -s -X POST \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H "Content-Type: application/json" \
    -d @- \
    "https://generativelanguage.googleapis.com/v1beta/models/${TARGET_MODEL}:generateContent")
  
  CANDIDATE_ANSWER=$(echo "$TEST_RESPONSE" | jq -r '.candidates[0].content.parts[0].text // "I do not know."')

  # --- Eval: Judge the Answer ---
  EVAL_SYSTEM="You are an impartial judge. Evaluate the Candidate Answer against the Ground Truth."
  EVAL_PROMPT="Question: $QUESTION
Ground Truth: $ANSWER
Candidate Answer: $CANDIDATE_ANSWER

Your Task:
1. Summarize the key points of the Candidate Answer.
2. Compare it to the Ground Truth.
3. specific failures:
    - Direct contradiction.
    - Missing the core 'gotcha' fact.
    - Technically correct but misleading emphasis (e.g., burying the real answer in a footnote or side comment).

Output strictly in JSON."
  
  EVAL_SCHEMA='{
    "type": "object",
    "properties": {
      "is_correct": { "type": "boolean", "description": "False if the answer is wrong, misleading, or buries the lead." },
      "summary": { "type": "string", "description": "Brief summary of what the candidate actually said." },
      "critique": { "type": "string", "description": "Why it is correct or incorrect, referencing the Ground Truth." }
    },
    "required": ["is_correct", "summary", "critique"]
  }'

  EVAL_BODY=$(jq -n \
    --arg system "$EVAL_SYSTEM" \
    --arg prompt "$EVAL_PROMPT" \
    --argjson schema "$EVAL_SCHEMA" \
    '{
      system_instruction: { parts: [{ text: $system }] },
      contents: [{ parts: [{ text: $prompt }] }],
      generationConfig: {
        responseMimeType: "application/json",
        responseJsonSchema: $schema
      }
    }')

  EVAL_RESPONSE=$(echo "$EVAL_BODY" | curl -s -X POST \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H "Content-Type: application/json" \
    -d @- \
    "https://generativelanguage.googleapis.com/v1beta/models/${EVALUATOR_MODEL}:generateContent")

  EVAL_JSON=$(echo "$EVAL_RESPONSE" | jq -r '.candidates[0].content.parts[0].text // empty')
  
  if [[ -z "$EVAL_JSON" ]]; then
    echo "Warning: Judge returned empty response for question: $QUESTION" >&2
    echo "Raw Response: $EVAL_RESPONSE" >&2
    continue
  fi

  IS_CORRECT=$(echo "$EVAL_JSON" | jq -r '.is_correct')
  SUMMARY=$(echo "$EVAL_JSON" | jq -r '.summary')
  CRITIQUE=$(echo "$EVAL_JSON" | jq -r '.critique')

  # Debug output to stderr
  echo "  Judge Verdict: Is Correct? $IS_CORRECT" >&2

  if [[ "$IS_CORRECT" == "false" ]]; then
    {
      echo "### Q: $QUESTION"
      echo "**A (Ground Truth):** $ANSWER"
      echo ""
      echo "**Analysis (Fail):** $SUMMARY however, $CRITIQUE"
      echo "<details><summary>Raw Target Response</summary>"
      echo ""
      echo "$CANDIDATE_ANSWER"
      echo ""
      echo "</details>"
      echo ""
    } >> "$VALIDATED_FILE"
  else
    {
      echo "### Q: $QUESTION"
      echo ""
      echo "**Analysis (Success):** $SUMMARY. Judge confirmed: $CRITIQUE"
      echo "<details><summary>Raw Target Response</summary>"
      echo ""
      echo "$CANDIDATE_ANSWER"
      echo ""
      echo "</details>"
      echo ""
    } >> "$REJECTED_FILE"
  fi

done

# -----------------------------------------------------------------------------
# STEP 4: Final Report
# -----------------------------------------------------------------------------

echo "# Validated Knowledge Gaps"
echo "Target Model: \`$TARGET_MODEL\`"
echo ""

if [[ -s "$VALIDATED_FILE" ]]; then
  echo "## ✅ Confirmed Unknowns"
  echo "The model FAILED to answer these correctly, proving the information is novel."
  echo ""
  cat "$VALIDATED_FILE"
else
  echo "## ✅ Confirmed Unknowns"
  echo "_None found. The model appears to know all generated facts._"
fi

echo ""
echo "---"
echo ""

if [[ -s "$REJECTED_FILE" ]]; then
  echo "## ❌ False Alarms (Model Answered Correctly)"
  echo "The model SUCCESSFULLY answered these, so they are not novel gaps."
  echo ""
  cat "$REJECTED_FILE"
fi